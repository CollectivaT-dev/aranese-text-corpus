{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc6c8d6",
   "metadata": {},
   "source": [
    "# Web to text corpus full pipeline\n",
    "\n",
    "This notebook processes publications published in the [Premsa Aranesa repository](https://ddd.uab.cat/collection/honsaran) to obtain a sentence segmented corpus. \n",
    "\n",
    "The processes involved are:\n",
    "\n",
    "1. Parse the pages of publication record URL (e.g. https://ddd.uab.cat/record/218748) list specified in a text file. \n",
    "2. Download all associated PDFs\n",
    "3. Extract each page in the PDF as an image\n",
    "4. Perform optical character recognition (OCR) using Tesseract\n",
    "5. Correct ortographic and formatting errors using Claude LLM (needs API key but can be skipped also)\n",
    "6. Extract sentences from each document, save them for each publication and merge them into a single corpus file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a9ca4",
   "metadata": {},
   "source": [
    "## Download PDFs and create download_summary.JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15fa842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba5fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_links(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if '.pdf' in href:\n",
    "            pdf_links.append(href)\n",
    "\n",
    "    pdf_links = [requests.compat.urljoin(url, link) for link in pdf_links]\n",
    "\n",
    "    return pdf_links\n",
    "\n",
    "def get_publication_name(soup):\n",
    "    meta_tag = soup.find('meta', {'name': 'dc.title'})\n",
    "    if meta_tag and 'content' in meta_tag.attrs:\n",
    "        return meta_tag['content']\n",
    "    return \"Unknown_Publication\"\n",
    "\n",
    "def extract_id_from_url(url):\n",
    "    parts = url.split('/')\n",
    "    if len(parts) > 5:\n",
    "        return parts[5]\n",
    "    return \"Unknown_ID\"\n",
    "\n",
    "def download_pdfs(pdf_links, directory, max_retries=3, wait_time=2):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    downloaded_files = []\n",
    "    for link in pdf_links:\n",
    "        file_name = os.path.join(directory, os.path.basename(link))\n",
    "        if os.path.exists(file_name):\n",
    "            print(f\"Already downloaded: {file_name}\")\n",
    "            downloaded_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "        success = False\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(link)\n",
    "                if response.status_code == 200:\n",
    "                    with open(file_name, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    downloaded_files.append(file_name)\n",
    "                    print(f\"Downloaded: {file_name}\")\n",
    "                    success = True\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Failed to download: {link} (status code: {response.status_code})\")\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Attempt {attempt + 1} failed with error: {e}\")\n",
    "            \n",
    "            if not success:\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "        if not success:\n",
    "            print(f\"Failed to download: {link} after {max_retries} attempts\")\n",
    "\n",
    "    return downloaded_files\n",
    "\n",
    "def process_repository_links(file_path):\n",
    "    results = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        urls = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the page: {response.status_code} for URL: {url}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        publication_name = get_publication_name(soup)\n",
    "        \n",
    "        pdf_links = extract_pdf_links(url)\n",
    "        \n",
    "        publication_id = extract_id_from_url(pdf_links[0])\n",
    "        \n",
    "        if pdf_links:\n",
    "            directory = os.path.join(\"docs\", publication_id)\n",
    "            pdf_directory = os.path.join(directory, \"pdf\")\n",
    "            downloaded_files = download_pdfs(pdf_links, pdf_directory)\n",
    "            results.append({\n",
    "                \"name\": publication_name,\n",
    "                \"id\": publication_id,\n",
    "                \"url\": url,\n",
    "                \"directory\": directory,\n",
    "                \"no_docs\": len(downloaded_files),\n",
    "                \"docs\" : sorted(downloaded_files)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"No PDF links found for URL: {url}\")\n",
    "\n",
    "    # Write results to JSON file with proper encoding for non-ASCII characters\n",
    "    with open(os.path.join(\"docs\", \"download_summary.json\"), 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Download summary written to {os.path.join('docs', 'download_summary.json')}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17149f39",
   "metadata": {},
   "source": [
    "### Download PDFs from publication URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4de575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: docs/vietot/pdf/vietot_a2018n3.pdf\n",
      "Downloaded: docs/vietot/pdf/vietot_a2017n1.pdf\n",
      "Downloaded: docs/arapai/pdf/arapai_a2019n1.pdf\n",
      "Download summary written to docs/download_summary.json\n"
     ]
    }
   ],
   "source": [
    "file_path = \"uab_repo_urls_demo.txt\"\n",
    "download_summary = process_repository_links(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43389051",
   "metadata": {},
   "source": [
    "#### Run this if reading download_summary.JSON from disk (skipping the cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7caaf6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 records\n"
     ]
    }
   ],
   "source": [
    "with open('docs/download_summary.json', 'r') as f:\n",
    "    download_summary = json.load(f)\n",
    "\n",
    "# Now download_summary is available for use\n",
    "print(len(download_summary), \"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ce4b6",
   "metadata": {},
   "source": [
    "## Image extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c068f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66c407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert PDF to images\n",
    "def pdf_to_images(pdf_path):\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path)\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {pdf_path} to images: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to save images to files with resolution check\n",
    "def save_image(image, image_path):\n",
    "    try:\n",
    "        buffer = io.BytesIO()\n",
    "        image.save(buffer, format=\"PNG\")\n",
    "        file_size = buffer.tell()\n",
    "\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(buffer.getvalue())\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving image to {image_path}: {e}\")\n",
    "\n",
    "def extract_images_from_documents(download_summary, overwrite_imgs=False):\n",
    "    total_no_images = 0\n",
    "    total_saved_images = 0\n",
    "    \n",
    "    for pub in download_summary:\n",
    "        print(pub['name'])\n",
    "\n",
    "        for doc_path in pub['docs']:\n",
    "            doc_name = doc_path.split('/')[-1]\n",
    "            print(doc_name, end=\" - \")\n",
    "\n",
    "            imgs_dir = os.path.join(pub['directory'], 'img', doc_name)\n",
    "\n",
    "            os.makedirs(imgs_dir, exist_ok=True)\n",
    "\n",
    "            # Check if the image directory is empty\n",
    "            ls_imgs_dir = [i for i in os.listdir(imgs_dir) if not i==\".DS_Store\" ]\n",
    "            if not overwrite_imgs and ls_imgs_dir:\n",
    "                print(\"Skipped (images already exist)\")\n",
    "                total_no_images += len(ls_imgs_dir)\n",
    "                continue\n",
    "        \n",
    "            # Extract images from pdf\n",
    "            images = pdf_to_images(doc_path)\n",
    "            print(len(images), \"images\", end=\" \")\n",
    "            \n",
    "            # Save images in parallel\n",
    "            def save_image_task(args):\n",
    "                image, image_path = args\n",
    "                save_image(image, image_path)\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                future_to_image = {executor.submit(save_image_task, (image, os.path.join(imgs_dir, f\"page_{i + 1}.png\"))): i for i, image in enumerate(images)}\n",
    "                for future in concurrent.futures.as_completed(future_to_image):\n",
    "                    total_saved_images += 1\n",
    "                    total_no_images += 1\n",
    "                    \n",
    "            print(\"saved\")\n",
    "\n",
    "        print()\n",
    "            \n",
    "    print(total_no_images, \"images in total from\", len(download_summary), \"documents\")\n",
    "    print(total_saved_images, \"new saved\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a774d3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Converting each pdf into images=======\n",
      "Vielha de toti : era revista dera gent de Vielha e Mijaran\n",
      "vietot_a2017n1.pdf - 16 images saved\n",
      "\n",
      "Aran un país\n",
      "arapai_a2019n1.pdf - 8 images saved\n",
      "\n",
      "24 images in total from 2 documents\n",
      "24 new saved\n",
      "Elapsed time: 6.079754114151001 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"=======Converting each pdf into images=======\")\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Convert each pdf into images\n",
    "extract_images_from_documents(download_summary, overwrite_imgs=False) ## overwrite_imgs True for debugging\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4536dc2",
   "metadata": {},
   "source": [
    "### Tesseract OCR + post-editing with Anthropic API\n",
    "\n",
    "Reminder: Place the anthropic API key into `.env` file as in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1799d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import re\n",
    "\n",
    "# Function to perform OCR on an image\n",
    "def ocr_image_to_text(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    text = pytesseract.image_to_string(image, lang='oci')  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590b9eb",
   "metadata": {},
   "source": [
    "#### Note: Correction step can be skipped. Don't execute cell below if so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a6b9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from environment variable\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "if api_key is None:\n",
    "    raise ValueError(\"No API key found. Please set the ANTHROPIC_API_KEY environment variable.\")\n",
    "\n",
    "anthropic_client = anthropic.Anthropic()\n",
    "\n",
    "instructions_detailed = (\n",
    "    \"You are tasked with fixing OCR output text in Aranese, respecting Aranese orthography, merging lines where necessary, \"\n",
    "    \"and producing a proper output. This task is crucial for preserving the integrity and readability of the material and \"\n",
    "    \"create a large text corpus for this language.\\n\\n\"\n",
    "\n",
    "    \"Follow these steps to process the text:\\n\\n\"\n",
    "\n",
    "    \"0. Only output the corrected text, without any introductory or additional phrases.\\n\\n\"\n",
    "\n",
    "    \"1. Analyze and correct Occitan Aranese characters and diacritical marks:\\n\"\n",
    "    \"   - Pay close attention to special characters and diacritical marks (e.g., à, è, ò, ï, ü).\\n\"\n",
    "    \"   - Ensure accurate spelling according to Occitan Aranese conventions.\\n\\n\"\n",
    "\n",
    "    \"2. Maintain sentence integrity:\\n\"\n",
    "    \"   - Examine each line to determine if it's a complete sentence or part of a continuing sentence.\\n\"\n",
    "    \"   - Join continuing sentences even if they span multiple lines visually.\\n\"\n",
    "    \"   - Look for grammatical clues such as:\\n\"\n",
    "    \"     a. Lack of final punctuation at the end of a line\\n\"\n",
    "    \"     b. Lines beginning with lowercase letters (unless it's a language-specific exception)\\n\"\n",
    "    \"     c. Incomplete grammatical structures that continue on the next line\\n\\n\"\n",
    "\n",
    "    \"3. Address justified paragraphs and hyphens:\\n\"\n",
    "    \"   - Remove hyphens within words that are divided across lines in justified text.\\n\"\n",
    "    \"   - Rejoin hyphenated words, placing the complete word at the end of the first line.\\n\"\n",
    "    \"   - Ensure that removing hyphens doesn't create unintended new words.\\n\\n\"\n",
    "\n",
    "    \"4. Preserve paragraph structure:\\n\"\n",
    "    \"   - Maintain paragraph breaks as they appear in the original text.\\n\"\n",
    "    \"   - Represent each new paragraph as a new line in the output.\\n\\n\"\n",
    "\n",
    "    \"5. Handle titles and headings:\\n\"\n",
    "    \"   - Preserve titles and headings as separate lines in the output.\\n\"\n",
    "    \"   - Do not join titles or headings with the following paragraph text.\\n\\n\"\n",
    "\n",
    "    \"6. Perform a final verification:\\n\"\n",
    "    \"   - Review the processed text to ensure:\\n\"\n",
    "    \"     a. All sentences are intact and properly joined.\\n\"\n",
    "    \"     b. Paragraphs and titles are correctly separated.\\n\"\n",
    "    \"     c. No artifacts from visual formatting remain.\\n\"\n",
    "    \"   - Double-check that the Occitan Aranese orthography is correctly preserved throughout.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a54bf27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ocr_text_with_api(ocr_text):\n",
    "    message = anthropic_client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0,\n",
    "        system=instructions_detailed,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": ocr_text\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return message.content[0].text\n",
    "\n",
    "# Custom sorting key to extract numbers from filenames\n",
    "def numerical_sort_key(value):\n",
    "    numbers = re.findall(r'\\d+', value)\n",
    "    return int(numbers[0]) if numbers else -1\n",
    "\n",
    "# Process to perform OCR and correct\n",
    "def do_ocr_and_fix(download_summary, skip_correction=False):\n",
    "    for pub in download_summary:\n",
    "        print(pub['name'])\n",
    "\n",
    "        imgs_dir = os.path.join(pub['directory'], 'img')\n",
    "\n",
    "        ocr_dir = os.path.join(pub['directory'], 'ocr')\n",
    "        os.makedirs(ocr_dir, exist_ok=True)\n",
    "\n",
    "        fix_dir = os.path.join(pub['directory'], 'fix')\n",
    "        os.makedirs(fix_dir, exist_ok=True)\n",
    "\n",
    "        # Iterate over each document in the publication\n",
    "        for doc_name in os.listdir(imgs_dir):\n",
    "            doc_img_dir = os.path.join(imgs_dir, doc_name)\n",
    "            doc_ocr_dir = os.path.join(ocr_dir, doc_name)\n",
    "            doc_fix_dir = os.path.join(fix_dir, doc_name)\n",
    "\n",
    "            # Skip non-directory files\n",
    "            if not os.path.isdir(doc_img_dir):\n",
    "                continue\n",
    "\n",
    "            print(\"-\", doc_name)\n",
    "\n",
    "            # Ensure the ocr and fix subdirectory for the document exists\n",
    "            os.makedirs(doc_ocr_dir, exist_ok=True)\n",
    "            os.makedirs(doc_fix_dir, exist_ok=True)\n",
    "\n",
    "            # List all images in the document's image directory\n",
    "            for img_name in sorted(os.listdir(doc_img_dir), key=numerical_sort_key):\n",
    "                img_path = os.path.join(doc_img_dir, img_name)\n",
    "                ocr_path = os.path.join(doc_ocr_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "                fix_path = os.path.join(doc_fix_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "\n",
    "                if os.path.isfile(img_path) and img_path.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    print(\"--\", img_name, end=\"\")\n",
    "                    print(\" OCR\", end=\"\")\n",
    "\n",
    "                    if not os.path.isfile(ocr_path):\n",
    "                        OCR_text = ocr_image_to_text(img_path)\n",
    "\n",
    "                        # Write OCR text to file\n",
    "                        with open(ocr_path, 'w') as f:\n",
    "                            f.write(OCR_text)\n",
    "                        print(\" ✅\", end = \"\")\n",
    "                    else:\n",
    "                        with open(ocr_path, 'r') as f:\n",
    "                            OCR_text = f.read()\n",
    "                        print(\" ✔️\", end = \"\")\n",
    "\n",
    "                    #Now send to Anthropic API to fix or do manual fixing\n",
    "                    print(\" FIX\", end=\"\")\n",
    "                    if not os.path.isfile(fix_path):\n",
    "                        if skip_correction:\n",
    "                            fixed_text = OCR_text\n",
    "                        else:\n",
    "                            fixed_text = fix_ocr_text_with_api(OCR_text)\n",
    "\n",
    "                        # Write fixed OCR text to file\n",
    "                        with open(fix_path, 'w') as f:\n",
    "                            f.write(fixed_text)\n",
    "                        print(\" ✅\")\n",
    "                    else: \n",
    "                        print(\" ✔️\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89fa46a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========Performing OCR and correction===========\n",
      "Vielha de toti : era revista dera gent de Vielha e Mijaran\n",
      "- vietot_a2017n1.pdf\n",
      "-- page_1.png OCR ✔️ FIX ✅\n",
      "-- page_2.png OCR ✔️ FIX ✔️\n",
      "-- page_3.png OCR ✔️ FIX ✔️\n",
      "-- page_4.png OCR ✔️ FIX ✔️\n",
      "-- page_5.png OCR ✔️ FIX ✔️\n",
      "-- page_6.png OCR ✔️ FIX ✔️\n",
      "-- page_7.png OCR ✔️ FIX ✔️\n",
      "-- page_8.png OCR ✔️ FIX ✔️\n",
      "-- page_9.png OCR ✔️ FIX ✔️\n",
      "-- page_10.png OCR ✔️ FIX ✔️\n",
      "-- page_11.png OCR ✔️ FIX ✔️\n",
      "-- page_12.png OCR ✔️ FIX ✔️\n",
      "-- page_13.png OCR ✔️ FIX ✔️\n",
      "-- page_14.png OCR ✔️ FIX ✔️\n",
      "-- page_15.png OCR ✔️ FIX ✔️\n",
      "-- page_16.png OCR ✔️ FIX ✔️\n",
      "\n",
      "Aran un país\n",
      "- arapai_a2019n1.pdf\n",
      "-- page_1.png OCR ✔️ FIX ✅\n",
      "-- page_2.png OCR ✔️ FIX ✔️\n",
      "-- page_3.png OCR ✔️ FIX ✔️\n",
      "-- page_4.png OCR ✔️ FIX ✔️\n",
      "-- page_5.png OCR ✔️ FIX ✔️\n",
      "-- page_6.png OCR ✔️ FIX ✔️\n",
      "-- page_7.png OCR ✔️ FIX ✔️\n",
      "-- page_8.png OCR ✔️ FIX ✔️\n",
      "\n",
      "Elapsed time: 6.778980255126953 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"===========Performing OCR and correction===========\")\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform OCR and correction\n",
    "do_ocr_and_fix(download_summary, skip_correction=False)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ad5d0",
   "metadata": {},
   "source": [
    "### Create sentence segmented corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55612047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import re\n",
    "import sys\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "END_PUNCS = [\"!\", '.', '?', '.\"', '?\"', '!\"', \"…\", \":\"]\n",
    "DOC_FORMATS = [\".txt\"]\n",
    "\n",
    "def ellipsis_split(text):\n",
    "    text = re.sub('\\.\\.\\.', '…', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    sents = []\n",
    "    currsent = \"\"\n",
    "    split_points = []\n",
    "    sent_begin = 0\n",
    "    for i, c in enumerate(text):\n",
    "        if c == \"…\":\n",
    "            if i+1 < len(text) and text[i+1].islower():\n",
    "                continue\n",
    "            elif i+2 < len(text) and text[i+2].islower():\n",
    "                continue\n",
    "            else:\n",
    "                sents.append(text[sent_begin:i+1])\n",
    "                sent_begin = i + 1\n",
    "        if i == len(text) - 1:\n",
    "            sents.append(text[sent_begin:i+1])\n",
    "    return sents\n",
    "\n",
    "def is_sent(text):\n",
    "    return any(text.endswith(punc) for punc in END_PUNCS) and len(text) > 3 and text[0].isupper() and \"__\" not in text and \"…………\" not in text\n",
    "\n",
    "def clean_sent(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub(\"^– \", '', text)\n",
    "    text = re.sub(\"^- \", '', text)\n",
    "    text = re.sub(\"^\\(\", '', text)\n",
    "    text = re.sub(\"\\)$\", '', text)\n",
    "    text = re.sub(\"^»[a-z]\\) \", '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def parse_sents(text):\n",
    "    lines_in_text = [t for t in text.split(\"\\n\") if t]\n",
    "    sents = []\n",
    "    \n",
    "    for line in lines_in_text:\n",
    "        sent_candidates = sent_tokenize(line.strip())\n",
    "        clean_sentences = [clean_sent(s) for s in sent_candidates]\n",
    "        sents.extend(clean_sentences)\n",
    "        \n",
    "    return sents\n",
    "\n",
    "def make_corpus(download_summary, general_corpus_path = 'docs/general-corpus.txt', sent_corpus_path = \"docs/sentence-corpus.txt\"):\n",
    "    all_publication_lines = []\n",
    "    complete_sents = []\n",
    "\n",
    "    for pub in download_summary:\n",
    "        print(pub['name'])\n",
    "\n",
    "        fix_dir = os.path.join(pub['directory'], 'fix')\n",
    "        corpus_file_name = f\"{pub['id']}-corpus.txt\"\n",
    "        corpus_path = os.path.join(pub['directory'], corpus_file_name)\n",
    "\n",
    "        all_sents = []\n",
    "\n",
    "        for doc_name in os.listdir(fix_dir):\n",
    "            doc_dir = os.path.join(fix_dir, doc_name)\n",
    "            if not os.path.isdir(doc_dir):\n",
    "                continue\n",
    "\n",
    "            for file_name in os.listdir(doc_dir):\n",
    "                if file_name == '.DS_Store':\n",
    "                    continue\n",
    "\n",
    "                file_path = os.path.join(doc_dir, file_name)\n",
    "                if os.path.splitext(file_name)[1] in DOC_FORMATS:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        text = file.read()\n",
    "\n",
    "                    sents = parse_sents(text)\n",
    "                    all_sents.extend(sents)\n",
    "                    complete_sents.extend([s for s in sents if is_sent(s)])\n",
    "                    \n",
    "\n",
    "        # Write all sentences to the publication corpus file\n",
    "        with open(corpus_path, 'w', encoding='utf-8') as f:\n",
    "            for sent in all_sents:\n",
    "                f.write(sent + '\\n')\n",
    "\n",
    "        # Add to the general corpus\n",
    "        all_publication_lines.extend(all_sents)\n",
    "\n",
    "        # Print the number of sentences and words for each publication\n",
    "        word_count = sum(len(sent.split()) for sent in all_sents)\n",
    "        print(f\"{len(all_sents)} lines, {word_count} words\")\n",
    "        print(f\"Corpus written to {corpus_path}\")\n",
    "        print(\"------------------------------------\")\n",
    "\n",
    "    # Write the general corpus\n",
    "    with open(general_corpus_path, 'w', encoding='utf-8') as f:\n",
    "        for sent in all_publication_lines:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    # Print the number of sentences and words for the general corpus\n",
    "    general_word_count = sum(len(sent.split()) for sent in all_publication_lines)\n",
    "    print(\"====================================\")\n",
    "    print(f\"General corpus written to {general_corpus_path}\")\n",
    "    print(f\"{len(all_publication_lines)} lines, {general_word_count} words\")\n",
    "    \n",
    "    # Write the sentence-ensured corpus\n",
    "    with open(sent_corpus_path, 'w', encoding='utf-8') as f:\n",
    "        for sent in complete_sents:\n",
    "            f.write(sent + '\\n')\n",
    "            \n",
    "    print(f\"Sentence corpus written to {general_corpus_path}\")\n",
    "    sentenced_word_count = sum(len(sent.split()) for sent in complete_sents)\n",
    "    print(f\"{len(complete_sents)} lines, {sentenced_word_count} words\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a98b79b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========Generating corpora===========\n",
      "Vielha de toti : era revista dera gent de Vielha e Mijaran\n",
      "1862 lines, 11617 words\n",
      "Corpus written to docs/vietot/vietot-corpus.txt\n",
      "------------------------------------\n",
      "Aran un país\n",
      "574 lines, 3084 words\n",
      "Corpus written to docs/arapai/arapai-corpus.txt\n",
      "------------------------------------\n",
      "====================================\n",
      "General corpus written to docs/general-corpus.txt\n",
      "2436 lines, 14701 words\n",
      "Sentence corpus written to docs/general-corpus.txt\n",
      "49 lines, 220 words\n",
      "Elapsed time: 0.09957480430603027 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"===========Generating corpora===========\")\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform OCR and correction\n",
    "make_corpus(download_summary)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd049068",
   "metadata": {},
   "source": [
    "# PIPELINE ENDS HERE\n",
    "\n",
    "Rest of this notebook is for auxiliary tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15cfd1",
   "metadata": {},
   "source": [
    "## OCR comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb27304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate WER and print differences\n",
    "def compare_texts(ocr_text, gold_text):\n",
    "    # Manual transformation to lowercase and strip extra spaces\n",
    "    ocr_text = ocr_text.lower().strip()\n",
    "    gold_text = gold_text.lower().strip()\n",
    "\n",
    "    # Calculate CER\n",
    "    cer = jiwer.cer(gold_text, ocr_text)\n",
    "    \n",
    "    # Print WER\n",
    "    print(f\"Character Error Rate (CER): {cer:.2f}\\n\")\n",
    "    \n",
    "    # Print differences\n",
    "    diff = difflib.ndiff(ocr_text.split(), gold_text.split())\n",
    "    count = 0\n",
    "    diffs_str = \"\"\n",
    "    for line in diff:\n",
    "        if line.startswith('- '):\n",
    "            diffs_str += line[2:] + ' / '\n",
    "        elif line.startswith('+ '):\n",
    "            count += 1\n",
    "            diffs_str += line[2:] + '\\n'\n",
    "            \n",
    "    print(f\"OCR / GOLD ({count}/{len(gold_text.split())}):\")\n",
    "    print(diffs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs_play/page_3_haiku.txt', 'r') as f:\n",
    "    postfixed_text = f.read()\n",
    "    \n",
    "with open('docs_play/page_3_sonnet.txt', 'r') as f:\n",
    "    direct_text = f.read()\n",
    "    \n",
    "compare_texts(postfixed_text, direct_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb33f81",
   "metadata": {},
   "source": [
    "## Sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed12de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'docs/vietot/img/vietot_a2017n1.pdf/page_6.png'\n",
    "\n",
    "base64_image = image_to_base64(img_path)\n",
    "\n",
    "OCR_text = get_text_from_image_with_api(base64_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcdec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OCR_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def fix_ocr_text_formatting(text):\n",
    "    # Replace hyphenated line breaks with nothing (merge words split by hyphens)\n",
    "    text = re.sub(r'-\\n', '', text)\n",
    "    \n",
    "    # Preserve double line breaks and replace single line breaks within paragraphs with a space\n",
    "#     text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    \n",
    "#     # Replace multiple spaces with a single space\n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "#     # Restore double line breaks as single line breaks\n",
    "#     text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    \n",
    "    # Strip leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61103bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'docs/vietot/img/vietot_a2017n1.pdf/page_6.png'\n",
    "\n",
    "ocr_text = ocr_image_to_text(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8763eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fix_ocr_text_formatting(ocr_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dfb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs_play/page_6_nolang.txt', 'r') as f:\n",
    "    postfixed_text = f.read()\n",
    "    \n",
    "with open('docs_play/page_6_fra.txt', 'r') as f:\n",
    "    direct_text = f.read()\n",
    "    \n",
    "compare_texts(postfixed_text, direct_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba78af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_texts(postfixed_text, direct_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011b1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
